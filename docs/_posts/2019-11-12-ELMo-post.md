---
layout: post
title: Learning ELMo Easy as Tickling Some Monster
---

This should be the begininign of the ELMo post, now I'm not super sure how this works, so lets try a bunch of stuff


## Big Idea

If you had to summarize ELMo in one sentence, an appropriate description would be that ELMo is a deep learning model that is used to learn features about text data. By learning features using the context of the phrase and the characteristics within each word, ELMo outputs features that can be used for subsequent NLP tasks. Hence, ELMo works as a model that learns *distributed word representations*/*word embeddings*.


# Model

## Building Blocks

In addition to using some basic deep learning design choices (residual networks, dropout and *l-2* regularization during training), the network has the following two building blocks:
- *Character Convolutions*: By using character convolutions, the model gets context-independent representations of each word before going deeper in the model. Using these convolutions the model generalize characteristics of similar words (e.g. "read" and "reading").
- *Bidirectional LSTMs*: By using these, the model learns the forward and backwards *language model* probabilities of the words. This ensures that the context of the word affects its embedding.


## Example Walkthrough

Suppose we want to obtain the ELMo embedding of the sentence "The cat in the hat".

### Character convolution

use word, use character conv in each word, have cat and hat be similar

**INSERT PIC OF F**

### Forward LSTM

use things to get features after doing sequential thingy

**INSERT PIC OF F**

### Backwards LSTM

same as forward but move it backwards

**INSERT PIC OF F**

### Residual Connection and Second biLSTM

concatenate stuff and then pass stuff to second bilstm

**INSERT PIC OF F**

### Forming the ELMo Vector

concatenate all vectors

**INSERT PIC OF F**

## Overview of Architecture

Putting it all together, ELMo looks like the following:

**INSERT PIC OF F**

The specific dimensions of the architecture are:
- Character convolutions of 
- 2 biLSTM layers, each with 4096 units that project into 512 dimensions. The second layer 


## ELMo Training

Mention dataset of billion wor dbenchmark
Mention stuff about the training set up

(For more details on the training, you can consult the script `bin/train_elmo.py` at the ELMo Github Repo https://github.com/allenai/bilm-tf)


# Task-specific ELMo

mention how by having features now you can do whatever

## Question Answering

example of what the paper does with question answering, and have an example setup


# Paper Results

show table with results and bried discussion of each, all in original paper


# Further Work with ELMo

have 2-3 follow up works that have used elmo successfully


## Atomic


They used elmo to generate the features to do something (not sure what but i can read up)

https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4160


## In contests and whatnot
### Suicide Reddit

Some of hte teams used ELMo for their feature generation

https://www.aclweb.org/anthology/W19-3003.pdf

### Irony spanish

Some of the teams used ELMo for their feature generation

http://ceur-ws.org/Vol-2421/IroSvA_overview.pdf

# More resources

list here the blogs that helped me and some extra links (especially the one that has the elmo repo)

https://jalammar.github.io/illustrated-bert/

https://allennlp.org/elmo

https://github.com/allenai/bilm-tf

https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/

# References


---
layout: post
title: BERT: Using Bidirectional Information for Language Understanding
---

Since being introduced to the world by Google in 2018, BERT has taken the natural language processing community by storm.




## Big Ideas / TLDR

- BERT stands for Bidirectional Encoder Representations for Transformers.
- Previous transformer models would use previous or future words when training the model to predict what word should come next. BERT uses both.
- Pre-trained using cloze task: hide a word, then use surrounding words to predict what the hidden word should be [REFERENCE NEEDED]
- Use pre-trained model and fine-tune it for many natural language processing tasks with supervised learning



## Model

BERT is a transformer network, and is built very similarly to the original transformer network described in [1]. For an overview of how these models work, please see <a href="">this blog post</a>.

### Architecture Components

High level overview of what the network is? 

[insert picture of the architecture here]

Quick stats of the small, basic BERT network:
- # of layers: 12
- # of nodes in hidden layers: 768
- # of self-attention heads: 12
- # of parameters: 110M

Describe each of these statistics, and what they mean. It is worth noting that it is possible to increase these statistics to create a larger, and more powerful, network that achieves better output results.


### Input Representation
- Uses WordPiece embeddings [2]

[insert picture of the input representation]

Provide explanation of the picture here. 


## BERT Training

BERT's training procedure can be divided into two parts:

1. Pre-Training (unsupervised)
2. Task-Specific training (supervised)

BERT utilizes transfer learning to first learn a generally good representation of context and language understanding, and then the weights/parameters in the network are optimized for a more specific task (like question answering, language translation, etc.) with a set of labeled data.

### Pre-Training

BERT's training procedure begins by figuring out a good set of initial weights for the parameters of the network before it is then fine-tuned for specific tasks. There are two pre-training procedures: **masked language modeling** and **next sentence prediction**.

#### Masked Language Model
This section will cover the **masked language modeling** task. What is the procedure? How is it better than using unidirectional predictions?


#### Next Sentence Prediction
This section will cover the **next sentence prediction** learning task. What does it do? Why is it used? 


#### Pre-Training Data Sets

This paper uses two datasets as part of its pre-training procedure:

1. BookCorpus (800M words) [REFERENCE NEEDED]
2. English Wikipedia (2,500M words) [REFERENCE NEEDED]


Why did they use these ones in particular? What do they look like? Show some examples. Which dataset is used for which pre-training task?

### Task-Specific Training

BERT can be tailored to handle a variety of specific applications within NLP. In this section, we will use one of these areas as an example to help us gain a better understanding of the procedure.


## Paper Results
This section will cover the empirical results that BERT achieved on a variety of standard tasks and testing datasets.


## State-of-the-Art BERT Extensions and Uses
SQuAD, Glue, and other results. Table of results.

## Resources
If you would like to get some hands-on experience with this model, the following resources will help you get started.

- <a href="https://github.com/google-research/bert">Google's BERT source code repository</a>


## References
1. Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems. 2017.
2. Wu, Yonghui, et al. "Google's neural machine translation system: Bridging the gap between human and machine translation." arXiv preprint arXiv:1609.08144 (2016).